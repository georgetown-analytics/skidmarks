{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"skidmark_small.bmp\" alt=\"skidmark\" align=\"right\" height=\"19%\" width=\"19%\">\n",
    "<center>\n",
    "<h2><span style=\"color:black;\">Using Telematic Data to Identify Increased Liability Drivers</span></h2><br>\n",
    "<br>\n",
    "<b>Team Name:</b><br>\n",
    "<br>\n",
    "SkidMarks<br>\n",
    "<br>\n",
    "<br>\n",
    "<b>Team Members:</b><br>\n",
    "<br>\n",
    "<a href=\"https://www.linkedin.com/pub/linwood-creekmore-iii/38/745/a21\">Linwood Creekmore</a><br>\n",
    "<a href=\"https://www.linkedin.com/pub/vikram-mittal/10/ba7/a81\">Vikram Mittal</a><br>\n",
    "<br>\n",
    "<br>\n",
    "_In partial fulfillment of the requirements for Georgetown University's Data Science Professional Certificate Program_<br>\n",
    "<br>\n",
    "April 2015\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h3><a name=\"Contents\">Table of Contents</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "<a href=\"#Abstract\">Abstract</a><br><br>\n",
    "<a href=\"#PixarPitch\">Pixar Pitch</a><br><br>\n",
    "<a href=\"#Introduction\">Introduction</a><br><br>\n",
    "<a href=\"#Data\">Data</a><br><br>\n",
    "<a href=\"#Issues\">Issues</a><br><br>\n",
    "<a href=\"#Methodology\">Methodology</a><br><br>\n",
    "<a href=\"#Discussion\">Discussion</a><br><br>\n",
    "<a href=\"#Challenges\">Challenges</a><br><br>\n",
    "<a href=\"#LessonsLearned\">Lessons Learned</a><br><br>\n",
    "<a href=\"#WinningTeam\">Winning Team</a><br><br>\n",
    "<a href=\"#FurtherResearch\">Further Research</a><br><br>\n",
    "<a href=\"#Conclusion\">Conclusion</a><br><br>\n",
    "<a href=\"#References\">References</a><br><br>\n",
    "<a href=\"#Appendices\">Appendices</a><br><br>\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><center>iPython formatting guidance</center> \n",
    "><center>http://nbviewer.ipython.org/github/NelisW/ComputationalRadiometry/blob/master/01-IPythonHintsAndTips.ipynb</center>\n",
    "><center> For those viewing this Notebook as a webpage, this service is provided by nbviewer:\n",
    "><center>http://nbviewer.ipython.org/</center>\n",
    "><center>Markdown Formatting Cheat Sheet</center>\n",
    "><center>http://assemble.io/docs/Cheatsheet-Markdown.html</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "picUrl = 'http://software.quest-global.com/wp-content/uploads/2014/11/connectedcar-logo.png'\n",
    "Embed  = Image(picUrl)\n",
    "display(Embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name=\"Abstract\">Abstract</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform and outline the steps taken to create a model to classify driver behavior based telematic data provided by AXA. Then, a prediction model is described to determine if a driver is likely to be a safe or unsafe driver based on a single trip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name=\"Introduction\">Introduction</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantitative risk management has been a cornerstone of the auto insurance industry for many years.  Traditionally, companies in the industry calculate risk-based premiums using driver history, demographics, geographic factors, and vehicle characteristics.  Driver history includes events such as accidents, moving violations, and license suspensions.  Demographic factors include age, gender, marital status, and occupation.  Geographic factors include traffic and weather patterns, as well as crime rates such as vandalism and theft.  Finally, vehicle characteristics include make, model, year, and mileage to derive a current value.  Insurance companies combine these various factors into an expected loss model to predict the probability of a claim and the size of a claim in the future.  While insurance providers clearly rely on a wealth of information to make pricing decisions, the data still has its limitations.  For example, driver history factors describe relatively infrequent, negative events.  Additionally, insurance companies typically only have access to three to five years of driver history data per state laws.  Progressive, a U.S. industry leader in data-driven policy making, found that key driving behaviors—like actual miles driven, braking, and time of day of driving—carry more than twice the predictive power of traditional insurance rating variables like those just described.  Furthermore, a white paper by Deloitte highlighted the mutually beneficial motivations behind this driving-data-craze in a 2012 white paper:\n",
    "\n",
    ">\"...the advantages accrue to virtually all sides. Insurance companies benefit from matching premiums more closely >with actual risk. Drivers benefit from the opportunity to lower their insurance rates by driving less or more >safely...\"\n",
    "\n",
    "Knowing that “more data often beats better models”, insurance companies are now looking to incorporate daily driving behavior into their premium pricing models to gain a competitive advantage.  You may be familiar with Progressive’s Snapshot commercials where customers choose to place a device in their vehicles that monitors the vehicle’s position and speed.  The idea is to reward customers with good driving habits by offering them discounted premiums.  With this information, Progressive can also identify customers with risky driving habits and then initiate a variety of mitigation options such as offering corrective guidance, increasing premiums, or even terminating coverage.  With the prevalence of GPS devices in vehicles, either as a standalone unit or built into a driver’s cell phone, insurance companies now have access to real-time driver behavior to allow for more accurate predictions of claims and expected losses than ever before.  Like Progressive Insurance, AXA Insurance is moving in the direction of capturing current driver data and has sponsored a Kaggle competition to obtain a top performing classification and prediction model.  The purpose of this paper is to outline the steps taken to create a model to classify driver behavior based telematic data provided by AXA.  Then, a prediction model is described to determine if a driver is likely to be a safe or unsafe driver based on a single trip.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name=\"Pixar Pitch\">The Pixar Pitch</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Once upon a time\n",
    "…auto insurance companies charged premiums based on accidents and moving violations. \n",
    "#### Every Day \n",
    "…drivers were being penalized for these infrequent, negative events.\n",
    "##### One Day\n",
    "...a company decided to capture the daily driving habits of its customers in its premium pricing model.\n",
    "###### Because of that\n",
    "…customers with good driving habits felt happier after being rewarded with lower premiums.\n",
    "####### Because of that\n",
    "…the insurance company experienced less customer turnover and more new customers applied for policies.\n",
    "######## Until finally \n",
    "...the insurance company had captured greater market share.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:clue;\">Insurance companies WANT your data!!!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AXA, the French multinational investment banking and insurance management firm, sponsored a Kaggle competition and provided a clean data file for each of 200 trips for 3,600+ drivers. Each file contained a list of anonymized vehicle positions at every second of the trip (i.e. each row represents the vehicle’s 2-dimensional position after one second). To protect the identity and location of the drivers, longitude and latitude coordinates are not provided so this data set cannot be matched up with coordinates of major roadways to determine speed limits, road signs, and traffic lights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver = raw_input(\"Enter a number between 1-3612:\\n>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trip = raw_input(\"Enter a number between 1-200:\\n>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = os.path.abspath(os.getcwd())\n",
    "pathtocsv = os.path.normpath(os.path.join(os.path.dirname(path),\"input\",\"test\",str(driver),str(trip)+\".csv\"))\n",
    "df = pd.read_csv(pathtocsv)\n",
    "print df[:15][['x','y']]\n",
    "\n",
    "df.plot(kind = 'scatter', x = 'x', y = 'y', figsize=(18,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Calculate the direction\n",
    "def getDirection(y,x):\n",
    "    direction_rad = math.atan2(y,x)\n",
    "    direction = math.degrees(direction_rad) \n",
    "    if direction < 0:\n",
    "        direction += 360\n",
    "    return direction \n",
    "\n",
    "def getCardinalDirection(direction):\n",
    "    carddir = ''\n",
    "    if direction >= 0 and direction <= 22.5:\n",
    "        carddir = 'East'\n",
    "    elif direction > 337.5 and direction <=360:\n",
    "        carddir = 'East' \n",
    "    elif direction > 22.5 and direction <= 67.5:\n",
    "        carddir = 'Northeast'\n",
    "    elif direction > 67.5 and direction <= 112.5:\n",
    "        carddir = 'North'\n",
    "    elif direction > 112.5 and direction <= 157.5:\n",
    "        carddir  = 'Northwest'\n",
    "    elif direction > 157.5 and direction <= 202.5:\n",
    "        carddir  = 'West'\n",
    "    elif direction > 202.5 and direction <= 247.5:\n",
    "        carddir = 'Southwest'\n",
    "    elif direction > 247.5 and direction <= 292.5:\n",
    "        carddir  = 'South'\n",
    "    elif direction > 292.5 and direction <= 337.5:\n",
    "        carddir  = 'Southeast'\n",
    "\n",
    "    return carddir\n",
    "\n",
    "print \"The cardinal direction is %r and the direction in degrees is %r\" %(getCardinalDirection(getDirection(11.2,-4.2)), getDirection(11.2,-4.2))\n",
    "\n",
    "val = pd.rolling_sum(df, window = 3)\n",
    "print val[2:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = os.path.abspath(os.getcwd())\n",
    "pathtocsv = os.path.normpath(os.path.join(os.path.dirname(path),\"output\",\"test\",str(driver), str(driver) + \"_\" +str(trip)+\".csv\"))\n",
    "normed_df = pd.read_csv(pathtocsv)\n",
    "print pathtocsv\n",
    "print normed_df[:15][[0,1,2,3]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name=\"Methodology\">Methodology and Tools</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Data Ingestion</i></b>. The original AXA data set was downloaded/warehoused as a zipped CSV file from the competition website.  The original file was stored in as raw a form as possible; each driver (total of 3612) had a directory with two-hundred (200) trips.  Each trip had an x and y column value for each REPORTED second of the trip.  Outside of the standard browser provided download tools and local operating system directories, no special tools were used for the ingestion phase of the Data Science Pipeline.   There was some experimentation with wget. Team SkidMarks initially planned to use to Postgres  as the Write Once Read Many (WORM) repository but scheduling, learning curves, and feature calculations (discussed later) consumed the bulk of the project time. Local folders on teammate computers served as the ingestion landing zone.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>Data Munging and Wrangling</i></b>. Using team-created python scripts, the data was normalized and replicated (added concatenated primary key with creation of trip_id and driver_id columns) to intuitively capture relationships within the data.  The os Python module was used for operating system interoperability and the csv Python module was used to read and write the normalized database structure to another directory for computation and analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>Computation and Analyses</i></b>. Naturally, this step consumed the bulk of the CAPSTONE project’s time and resources; feature creation was a coding, memory, and research intensive process.  The python Pandas library was used because it provides high-performance, easy-to-use data structures and data analysis tools within Python.  From simple x and y positional data, Team SkidMarks  used the pandas library to compute over 18 telematic and trip matching features:\n",
    "\n",
    "* Velocity\n",
    "\n",
    "* Average Velocity\n",
    "\n",
    "* Standard Deviation\n",
    "\n",
    "* Maximum\n",
    "\n",
    "* Acceleration\n",
    "\n",
    "* Average\n",
    "\n",
    "* Standard Deviation\n",
    "\n",
    "* Maximum\n",
    "\n",
    "* Distance\n",
    "\n",
    "* Increment traveled per second\n",
    "\n",
    "* Total distance traveled (sum of all increments)\n",
    "\n",
    "* Total displacement\n",
    "\n",
    "* Heading\n",
    "\n",
    "* Heading standard deviation\n",
    "\n",
    "* Heading in degrees\n",
    "\n",
    "* Heading as a cardinal direction\n",
    "\n",
    "* Maximum 1-second change in heading  for trip\n",
    "\n",
    "* Turns\n",
    "\n",
    "*  count of maneuvers greater than 45 degree change in direction over 3 second rolling window\n",
    "\n",
    "* Turn plus speed over 30 mph (aggressive turns)\n",
    "\n",
    "* Braking\n",
    "\n",
    "* Deceleration events lasting 3 seconds (braking)\n",
    "\n",
    "* Large deceleration events\n",
    "\n",
    "* Maximum deceleration events\n",
    "\n",
    "* Consecutive zero acceleration/velocity/increment travels events \n",
    "\n",
    "* Time\n",
    "\n",
    "* Total REPORTED trip time\n",
    "\n",
    "Base features were computed on a trip/per second basis.  Next, aggregate trip values, such as the average velocity over the duration of the trip, were computed for all driver files in the data store.    \n",
    "\n",
    "Despite several value/rule-based filters, Team SkidMarks did not completely remove outlier data.  “Hyperspace jumps” still remain as outliers in the data set because of  periods where travelers covered large distances in short times because of GPS signal loss.  However, existing filtering removed the bulk of the erroneous data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Modeling and Application</i></b>. Determining safe vs unsafe drivers is an unsupervised learning task.  Clustering with K-Means provides the best machine learning option to explore the dataset and will also help discover hidden structure or patterns in unlabeled training data.   The goal is to use cluster analysis to take a group of observed trips by drivers so that drivers with similar telematic and/or trip matching features such that members of the same group or cluster are more similar to each other by a given metric than they are to members of other clusters.  Team SkidMarks used the KMeans Clustering module within Python’s SciKit-Learn Machine Learning library.  The pandas, numpy, and Scipy  libraries were used with SciKit-Learn to ingest/preprocess/load data.\n",
    "\n",
    "The Silhouette Coefficient was used as a performance measure for the clusters.  The silhouette coefficient is a measure of the compactness and separation of the clusters.  The best value is 1 and the worst value is -1.\n",
    "\n",
    "KMeans clustered on train data, and a function, that, given train data, returned an array of integer labels corresponding to the different clusters.  Team SkidMarks experimented with using the learned features to build a supervised classifier, making this CAPSTONE a semi supervised learning problem in the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Reporting and Visualization</i></b>. Results were presented as visualizations using the matplotlib Python library.  A scatter matrix was initially used to explore the potential relationships between the calculated features.  Next, cluster visualizations were created to show the decision space and cluster groupings calculated via the KMeans clustering algorithm.  And finally, a simple fitted line plot was used to show the fitted classifier data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(os.path.normpath(os.path.join(os.path.dirname(path),'lin.csv')))\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = os.path.abspath(os.getcwd())\n",
    "pathtocsv = os.path.normpath(os.path.join(os.path.dirname(path),\"input\",\"test\",\"1\",\"200\" +\".csv\"))\n",
    "df = pd.read_csv(pathtocsv)\n",
    "df.plot(kind = 'scatter', x = 'x', y = 'y', figsize=(18,10))\n",
    "plt.ylim(-2000,650)\n",
    "plt.xlim(-4500,100)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def DisPlt(driver,trip):\n",
    "\n",
    "\tpath = os.path.abspath(os.getcwd())\n",
    "\tpathtocsv = os.path.normpath(os.path.join(os.path.dirname(path),\"output\",\"trip\",str(driver)+\"_\"+str(trip)+\".csv\"))\n",
    "\tdf = pd.read_csv(pathtocsv)\n",
    "\n",
    "\tinvestigation = str(raw_input(\"Enter a variable \\n>\"))\n",
    "\n",
    "\th = sorted([df[investigation]]) #sorted\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tfit = stats.norm.pdf(h, np.mean(h), np.std(h))  #this is a fitting indeed\n",
    "\n",
    "\tplt.plot(h,fit,'-o')\n",
    "\n",
    "\tplt.hist(h,normed=True)      #use this to draw histogram of your data\n",
    "\n",
    "\tplt.show()                   #use may also need add this\n",
    "\n",
    "###############################################################################\n",
    "# 'Main' Function\n",
    "############################################################################### \n",
    "\n",
    "\n",
    "DisPlt(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter matrix is a visualizaiton that helps to explore the \"character\" of the data; some of the feature comparisons may suggest potential correlations or relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "\n",
    "scatter_matrix(df2,alpha=0.5, figsize=(15,15),diagonal='kde')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The first goal is to group a set of drivers into the same group (called a cluster), where drivers within the group are more similar (in some sense or another) to each other than to those in other groups (clusters). The KMeans was used because the algorithm scales well to large data sets and is one of the easier algorithms to implement and return classes.  The initial clustering follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Some colors for later\n",
    "colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
    "colors = np.hstack([colors] * 20)\n",
    "  \n",
    "###\n",
    " #Initial data load from the aggregate file csv\n",
    "with open(os.path.normpath(os.path.join(os.path.dirname(path),'lin.csv'))) as in_data:\n",
    "    skid_data = pd.DataFrame.from_csv(in_data, sep=',')\n",
    "\n",
    "print list(enumerate(skid_data.columns))\n",
    "\n",
    "#Loading into the numpy array\n",
    "\n",
    "as_array = np.asfarray(skid_data[['Average Velocity (mph)','Max Velocity', 'Velocity Stdev','Average Acceleration (mph per s)', 'Max Acceleration (mph per s)', ' Acceleration Stdev','Displacement','Total Distance Traveled','Max Direction Change per sec', ' Direction Stdev','Time (s)', 'Turns', 'Aggressive Turns', 'Stops', 'Large Deceleration Events', 'Deceleration Events', 'Max Deceleration Event']])\n",
    "\n",
    "\n",
    "print skid_data.shape\n",
    "\n",
    "#number of groups\n",
    "n_clusters=4\n",
    "\n",
    "# preprocessing tricks\n",
    "imputer = Imputer(missing_values=\"NaN\", strategy=\"mean\")\n",
    "patched = imputer.fit_transform(as_array)\n",
    "patched = StandardScaler().fit_transform(patched)\n",
    "#patched = scale(patched, axis=0, with_mean=True)\n",
    "\n",
    "print patched\n",
    "\n",
    "\n",
    "\n",
    "#cluster data \n",
    "cluster = KMeans(n_clusters=n_clusters)\n",
    "cluster.fit(patched)\n",
    "\n",
    "#assigned grouped labels to the skid data\n",
    "labels = cluster.labels_\n",
    "skid_data[\"labels\"]=labels\n",
    "\n",
    "#pdict = create_ordered_dict(crime_data, \"labels\")\n",
    "'''\n",
    "# Fit the model with our algorithm\n",
    "cluster = MiniBatchKMeans(n_clusters=3)\n",
    "cluster.fit(as)\n",
    "'''\n",
    "# Make Predictions\n",
    "predictions = cluster.predict(patched)\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the clusters\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(patched[:, 0], patched[:, 5], color=colors[predictions].tolist(), s=5)\n",
    "centers = cluster.cluster_centers_\n",
    "center_colors = colors[:len(centers)]\n",
    "plt.scatter(centers[:, 0], centers[:, 5], s=169, c=center_colors,marker='x', linewidths=3,\n",
    "            color='white', zorder=10)\n",
    "plt.title('K-means clustering on Average Acceleration and the number of \\ncalculated turns in a trip'\n",
    "          'Centroids are marked with blue cross')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.ylabel('$Feature A$')\n",
    "plt.xlabel('$Feature B$')\n",
    "plt.figure(num=1, figsize=(18, 18), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "classified_data = cluster.labels_\n",
    "skid_data = skid_data.copy()\n",
    "print pd.Series(classified_data).mode()\n",
    "skid_data['Cluster Class'] = pd.Series(classified_data, index=skid_data.index)\n",
    "print \"This is what all of my work was for, this one little array.\\n\", classified_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A test on our clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "with open(os.path.normpath(os.path.join(os.path.dirname(path),'lin.csv'))) as in_data:\n",
    "    skid_data = pd.DataFrame.from_csv(in_data, sep=',')\n",
    "\n",
    "X = np.asfarray(skid_data[['Average Velocity (mph)','Turns','Max Velocity', 'Velocity Stdev','Average Acceleration (mph per s)', 'Max Acceleration (mph per s)', ' Acceleration Stdev','Displacement','Total Distance Traveled','Max Direction Change per sec', ' Direction Stdev','Time (s)', 'Turns', 'Aggressive Turns', 'Stops', 'Large Deceleration Events', 'Deceleration Events', 'Max Deceleration Event']])\n",
    "\n",
    "\n",
    "# Preprocessing tricks\n",
    "\n",
    "imputer = Imputer(missing_values=\"NaN\", strategy=\"mean\")\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for Telematic Data\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Clustered Drivers\")\n",
    "\n",
    "    # The vertical line for average silhoutte score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors)\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1],\n",
    "                marker='o', c=\"white\", alpha=1, s=200)\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "\n",
    "\n",
    "path = path = os.path.abspath(os.getcwd())\n",
    "\n",
    "# Some colors for later\n",
    "colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
    "colors = np.hstack([colors] * 20)\n",
    "\n",
    "###\n",
    " #load data from a CSV to a dataframe\n",
    "with open(os.path.normpath(os.path.join(os.path.dirname(path),'lin.csv'))) as in_data:\n",
    "    skid_data = pd.DataFrame.from_csv(in_data, sep=',')\n",
    "\n",
    "n_samples, n_features = skid_data.shape\n",
    "print skid_data.shape\n",
    "\n",
    "#skid_data=skid_data.fillna(value=-999)\n",
    "\n",
    "#load all numeric data into an array. The offense column from the crime data\n",
    "#is excluded\n",
    "as_array = np.asfarray(skid_data[['Average Velocity (mph)','Max Velocity', 'Velocity Stdev','Average Acceleration (mph per s)', 'Max Acceleration (mph per s)', ' Acceleration Stdev','Displacement','Total Distance Traveled','Max Direction Change per sec', ' Direction Stdev','Time (s)', 'Turns', 'Aggressive Turns', 'Stops', 'Large Deceleration Events', 'Deceleration Events', 'Max Deceleration Event']])\n",
    "\n",
    "#number of groups\n",
    "n_clusters=4\n",
    "\n",
    "\n",
    "#Correct missing data \n",
    "imputer = Imputer(missing_values=\"NaN\", strategy=\"mean\")\n",
    "patched = imputer.fit_transform(as_array)\n",
    "\n",
    "# Preprocessing tricks\n",
    "patched = StandardScaler().fit_transform(patched)\n",
    "#patched = scale(patched, axis=0, with_mean=True)\n",
    "\n",
    "#patched = preprocessing.normalize(patched, norm='l2')\n",
    "\n",
    "#min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#patched = min_max_scaler.fit_transform(patched)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#cluster data \n",
    "cluster = KMeans(n_clusters=n_clusters)\n",
    "cluster.fit(patched)\n",
    "\n",
    "\n",
    "\n",
    "# assigned grouped labels to the Skid data\n",
    "#labels = cluster.labels_\n",
    "#skid_data[\"labels\"]=labels\n",
    "'''\n",
    "# Fit the model with our algorithm\n",
    "cluster = MiniBatchKMeans(n_clusters=3)\n",
    "cluster.fit(patched)\n",
    "'''\n",
    "reduced_data = PCA(n_components=2).fit_transform(patched)\n",
    "\n",
    "\n",
    "kmeans = KMeans(init='k-means++', n_clusters=n_clusters, n_init=20)\n",
    "fit = kmeans.fit(reduced_data)\n",
    "predict = kmeans.predict(reduced_data)\n",
    "\n",
    "# Make Predictions\n",
    "labels = cluster.predict(patched)\n",
    "\n",
    "\n",
    "\n",
    "# array of indexes corresponding to classes around centroids, in the order of your dataset\n",
    "classified_data = kmeans.labels_\n",
    "prediction_data = labels\n",
    "\n",
    "#copy dataframe (may be memory intensive but just for illustration)\n",
    "skid_data = skid_data.copy()\n",
    "#print pd.Series(classified_data)\n",
    "#print pd.Series(prediction_data)\n",
    "skid_data['Predicted Class'] = pd.Series(prediction_data, index=skid_data.index)\n",
    "#print skid_data.describe()\n",
    "print cluster.labels_\n",
    "#print list(skid_data.columns)\n",
    "skid_data.plot( x = 'Average Acceleration (mph per s)', y = 'Predicted Class', kind = 'scatter')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Scoring to evaluate cluster performance\n",
    "\n",
    "# Silhouette Coefficient\n",
    "print \"We want scores close to 1 \\n\"\n",
    "\n",
    "SilouetteCoefficient = metrics.silhouette_score(patched, classified_data, metric='euclidean')\n",
    "'''\n",
    "\n",
    "AdjustRandIndex = metrics.adjusted_rand_score(classified_data, prediction_data)\n",
    "MutualInfoScore = metrics.adjusted_mutual_info_score(classified_data,prediction_data)\n",
    "HomogenietyScore = metrics.homogeneity_score(classified_data, prediction_data) \n",
    "CompletenessScore = metrics.completeness_score(classified_data, prediction_data)\n",
    "V_measure = metrics.v_measure_score(classified_data, prediction_data) \n",
    "'''\n",
    "\n",
    "\n",
    "'\\nThe Adjusted Rand index is %r\\nThe Mutual Information based score is %r\\nThe Homogeneity score is %r\\nThe completeness score is %r\\nThe V-measure score is %r\" % (SilouetteCoefficient,AdjustRandIndex,MutualInfoScore,HomogenietyScore,CompletenessScore,V_measure)'\n",
    "print \"The Silouette Coefficient score is %r...\\n\", SilouetteCoefficient\n",
    "\n",
    "#############\n",
    "#scikit-learn visualization example\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() + 1, reduced_data[:, 0].max() - 1\n",
    "y_min, y_max = reduced_data[:, 1].min() + 1, reduced_data[:, 1].max() - 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering on the snippet of Team Skidmarks dataset \\n(PCA-reduced data)'\n",
    "          'Centroids are marked with blue cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated in the figure above, the clusters are nearly indistiguishable when we reduce the dimensionality of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name=\"Challenges\">Challenges</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*“Hyperspace jumps” where the GPS signal was likely lost for a short period of time, causing the data to appear as if the vehicle traveled 1,000 mph for a period of time.\n",
    "*We later discovered that the vehicle’s heading, i.e. the direction it is traveling, can also change dramatically because of GPS errors.\n",
    "*To properly adjust for these GPS signal loss issues, smoothing techniques are needed.\n",
    "*To discourage trip matching, AXA inserted fake trips.  Need a method to programmatically identify and eliminate these trips.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name=\"Discussion\">Discussion</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data wrangling and data munging consume 90% of the time\n",
    "- Sci-kit learn documentation makes modeling and analysis easier\n",
    "- since our features are all calculated, our data science pipeline is a little different.  After each feature calculation, we need to revisit data wrangling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name=\"Conclusion\">Conclusion</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not broken out the key features, but they exist in our feature set based on the Kaggle Competiton Winner's forum posts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name=\"References\">References</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a bibliography or works cited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name=\"Appendices\">Appendices</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "any appendices that highlight key parts of your work (code snippets, data snippets, other graphs, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
